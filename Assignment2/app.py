import os
import sys
import subprocess
import streamlit as st

# Page configuration - MUST BE FIRST STREAMLIT COMMAND
st.set_page_config(
    page_title="ML Assignment 2",
    layout="wide",
    initial_sidebar_state="collapsed"
)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Check if models exist - using all_models.pkl
models_dir = 'models'
required_model_files = [
    'all_models.pkl'  # Single file containing all models
]

# Get absolute paths
base_dir = os.path.dirname(os.path.abspath(__file__))
models_full_path = os.path.join(base_dir, models_dir)

# Check which files exist
existing_files = []
missing_files = []
for f in required_model_files:
    file_path = os.path.join(models_full_path, f)
    if os.path.exists(file_path):
        existing_files.append(f)
    else:
        missing_files.append(f)

models_exist = len(missing_files) == 0

# Debug info in sidebar (collapsible)
with st.sidebar:
    with st.expander("üîç Debug Info"):
        st.write(f"**Base directory:** `{base_dir}`")
        st.write(f"**Models directory:** `{models_full_path}`")
        st.write(f"**Models exist:** {models_exist}")
        if existing_files:
            st.write(f"**‚úÖ Found:** {', '.join(existing_files)}")
        if missing_files:
            st.write(f"**‚ùå Missing:** {', '.join(missing_files)}")

if not models_exist:
    st.info("üîß Training models for the first time... This may take a few minutes.")
    
    # Create models directory if it doesn't exist
    os.makedirs(models_dir, exist_ok=True)
    
    try:
        # Determine correct base path
        # If app.py is in Assignment2 folder, we're already in the right place
        # If we're in the repo root, we need to go into Assignment2
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # First run data preprocessing
        st.info("Step 1/2: Running data preprocessing...")
        preprocess_script = os.path.join(base_dir, 'src', 'data_preprocessing.py')
        
        preprocess_result = subprocess.run(
            [sys.executable, preprocess_script],
            capture_output=True,
            text=True,
            cwd=base_dir
        )
        
        if preprocess_result.returncode != 0:
            st.error("‚ùå Data preprocessing failed!")
            with st.expander("View Error Details"):
                st.code(f"Script path attempted: {preprocess_script}")
                st.code(f"Working directory: {base_dir}")
                st.code(preprocess_result.stderr)
                st.code(preprocess_result.stdout)
            st.stop()
        
        st.success("‚úÖ Data preprocessing completed!")
        
        # Then run model training
        st.info("Step 2/2: Training models...")
        training_script = os.path.join(base_dir, 'src', 'model_training.py')
        
        training_result = subprocess.run(
            [sys.executable, training_script],
            capture_output=True,
            text=True,
            cwd=base_dir
        )
        
        if training_result.returncode == 0:
            st.success("‚úÖ Models trained successfully!")
            st.info("üîÑ The application will reload automatically in 3 seconds...")
            
            # Use HTML meta refresh to reload the page
            st.markdown("""
                <meta http-equiv="refresh" content="3">
                <script>
                    setTimeout(function() {
                        window.location.reload();
                    }, 3000);
                </script>
            """, unsafe_allow_html=True)
            
            # Also provide manual button as backup
            if st.button("üîÑ Refresh Application Now", type="primary", key="manual_refresh"):
                st.markdown('<meta http-equiv="refresh" content="0">', unsafe_allow_html=True)
            
            st.stop()
        else:
            st.error("‚ùå Model training failed!")
            with st.expander("View Error Details"):
                st.code(training_result.stderr)
                st.code(training_result.stdout)
            st.stop()
            
    except Exception as e:
        st.error(f"‚ùå Error during setup: {str(e)}")
        import traceback
        with st.expander("View Full Error"):
            st.code(traceback.format_exc())
        st.stop()

# Import after models are confirmed to exist
from src.utils import load_models, preprocess_uploaded_data, predict_with_model, calculate_metrics
from styles import get_custom_css, COLORS, CHART_COLORS
from constants import (APP_CONFIG, MODEL_DESCRIPTIONS, METRIC_INFO, METRICS_TO_HIGHLIGHT,METRIC_FORMAT, CHART_CONFIG, 
                       ANALYSIS_TEXT, UI_MESSAGES,TEST_DATA_REQUIREMENTS, FILE_UPLOAD_INFO, AUC_THRESHOLDS, 
                       TAB_NAMES, SECTION_HEADERS, PREDICTION_LABELS)

import warnings
warnings.filterwarnings('ignore')

# Apply custom CSS
st.markdown(get_custom_css(), unsafe_allow_html=True)

def plot_confusion_matrix(cm, title="Confusion Matrix"):
    """Plot confusion matrix with better styling"""
    config = CHART_CONFIG['confusion_matrix']
    fig, ax = plt.subplots(figsize=config['figsize'])
    sns.heatmap(cm, annot=True, fmt='d', cmap=config['cmap'], ax=ax,
                xticklabels=config['labels'], yticklabels=config['labels'],
                cbar_kws={'label': 'Count'}, annot_kws={'size': 14, 'weight': 'bold'})
    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
    plt.tight_layout()
    return fig

def display_metric_cards(metrics):
    """Display metrics in beautiful cards"""
    cols = st.columns(6)
    
    for col, (label) in zip(cols, METRIC_INFO):
        with col:
            value = metrics[label]
            st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{value:.4f}</div>
                    <div class="metric-label">{label}</div>
                </div>
            """, unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)

def render_training_results(models, scaler, feature_names, saved_metrics, saved_cm):
    """Render the training results tab"""
    st.markdown(f'<h2 class="section-header">{SECTION_HEADERS["performance_overview"]}</h2>', 
                unsafe_allow_html=True)
    
    # Best model highlight
    best_model_idx = saved_metrics['Accuracy'].idxmax()
    best_model_name = saved_metrics.loc[best_model_idx, 'Model']
    best_accuracy = saved_metrics.loc[best_model_idx, 'Accuracy']
    best_auc = saved_metrics.loc[best_model_idx, 'AUC']
    
    st.markdown(f"""
        <div class="success-box">
            <h3 style="margin:0; color:#16a34a;">üèÜ Best Performing Model</h3>
            <p style="font-size:1.3rem; margin:0.5rem 0; font-weight:600; color:#15803d;">
                {best_model_name}
            </p>
            <p style="margin:0; color:#166534;">
                Accuracy: <strong>{best_accuracy:.4f}</strong> | AUC: <strong>{best_auc:.4f}</strong>
            </p>
        </div>
    """, unsafe_allow_html=True)
    
    # Metrics Comparison Table
    st.markdown(f'<h3 class="section-header">{SECTION_HEADERS["metrics_table"]}</h3>', 
                unsafe_allow_html=True)
    
    def highlight_max(s):
        is_max = s == s.max()
        return ['background-color: #d4edda; font-weight: bold' if v else '' for v in is_max]
    
    styled_df = saved_metrics.style.apply(highlight_max, subset=METRICS_TO_HIGHLIGHT)\
                                  .format(METRIC_FORMAT)
    
    st.dataframe(styled_df, use_container_width=True, height=280)
    
    # Model Analysis - All Models
    st.markdown(f'<h3 class="section-header">{SECTION_HEADERS["model_analysis"]}</h3>', 
                unsafe_allow_html=True)
    
    # Display all models
    for model_name in models.keys():
        st.markdown(f"---")
        st.markdown(f"### {model_name}")
        
        # Get metrics for this model
        model_metrics = saved_metrics[saved_metrics['Model'] == model_name].iloc[0]
        
        # Display metrics in cards
        display_metric_cards(model_metrics)
        
        # Model description and confusion matrix
        col1, col2 = st.columns([1.2, 1])
        
        with col1:
            st.markdown("#### Model Description & Insights")
            info = MODEL_DESCRIPTIONS[model_name]
            
            st.markdown(f"""
                <div class="info-box">
                    <p style="margin:0; font-size:1rem; line-height:1.6;">
                        <strong>Description:</strong><br>{info['desc']}
                    </p>
                </div>
            """, unsafe_allow_html=True)
            
            st.markdown(f"**Strengths:**  \n{info['strengths']}")
            st.markdown(f"**Weaknesses:**  \n{info['weaknesses']}")
            st.markdown(f"**Best Use Case:**  \n{info['use_case']}")
        
        with col2:
            st.markdown("#### Confusion Matrix")
            if model_name in saved_cm:
                fig = plot_confusion_matrix(saved_cm[model_name], f"{model_name}")
                st.pyplot(fig)
                plt.close()
    
    # Performance Visualizations
    st.markdown(f'<h3 class="section-header">{SECTION_HEADERS["performance_viz"]}</h3>', 
                unsafe_allow_html=True)
    
    viz_col1, viz_col2 = st.columns(2)
    
    with viz_col1:
        st.markdown("#### üéØ Model Accuracy Comparison")
        st.markdown(f'<div class="warning-box">{ANALYSIS_TEXT["accuracy"]}</div>', 
                   unsafe_allow_html=True)
        
        config = CHART_CONFIG['accuracy_comparison']
        fig1, ax1 = plt.subplots(figsize=config['figsize'])
        colors = [COLORS['best_model'] if model == best_model_name else COLORS['default_model']
                 for model in saved_metrics['Model']]
        bars = ax1.bar(saved_metrics['Model'], saved_metrics['Accuracy'], 
                      color=colors, edgecolor='black', linewidth=1.5)
        ax1.set_ylabel(config['ylabel'], fontsize=12, fontweight='bold')
        ax1.set_xlabel(config['xlabel'], fontsize=12, fontweight='bold')
        ax1.set_title(config['title'], fontsize=14, fontweight='bold', pad=15)
        ax1.set_ylim([saved_metrics['Accuracy'].min() - 0.05, 
                     saved_metrics['Accuracy'].max() + 0.05])
        plt.xticks(rotation=45, ha='right')
        
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}',
                    ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        plt.tight_layout()
        st.pyplot(fig1)
    
    with viz_col2:
        st.markdown("#### üìä Multi-Metric Performance Radar")
        st.markdown(f'<div class="warning-box">{ANALYSIS_TEXT["multi_metric"]}</div>', 
                   unsafe_allow_html=True)
        
        config = CHART_CONFIG['multi_metric']
        fig2, ax2 = plt.subplots(figsize=config['figsize'])
        metrics_to_plot = saved_metrics.set_index('Model')[config['metrics']]
        metrics_to_plot.plot(kind='bar', ax=ax2, width=0.8, color=CHART_COLORS['metrics'])
        ax2.set_ylabel(config['ylabel'], fontsize=12, fontweight='bold')
        ax2.set_xlabel(config['xlabel'], fontsize=12, fontweight='bold')
        ax2.set_title(config['title'], fontsize=14, fontweight='bold', pad=15)
        ax2.legend(loc='lower right', framealpha=0.9, fontsize=10)
        ax2.set_ylim(config['ylim'])
        plt.xticks(rotation=45, ha='right')
        plt.grid(axis='y', alpha=0.3, linestyle='--')
        plt.tight_layout()
        st.pyplot(fig2)
    
    # AUC Comparison
    st.markdown("#### üé™ AUC Score Analysis")
    st.markdown(f'<div class="warning-box">{ANALYSIS_TEXT["auc"]}</div>', 
               unsafe_allow_html=True)
    
    config = CHART_CONFIG['auc_comparison']
    fig3, ax3 = plt.subplots(figsize=config['figsize'])
    colors_auc = [CHART_COLORS['auc_excellent'] if auc >= AUC_THRESHOLDS['excellent'] 
                  else CHART_COLORS['auc_good'] if auc >= AUC_THRESHOLDS['good'] 
                  else CHART_COLORS['auc_poor']
                  for auc in saved_metrics['AUC']]
    bars_auc = ax3.barh(saved_metrics['Model'], saved_metrics['AUC'], 
                       color=colors_auc, edgecolor='black', linewidth=1.5)
    ax3.set_xlabel(config['xlabel'], fontsize=12, fontweight='bold')
    ax3.set_title(config['title'], fontsize=14, fontweight='bold', pad=15)
    ax3.set_xlim(config['xlim'])
    
    for bar in bars_auc:
        width = bar.get_width()
        ax3.text(width, bar.get_y() + bar.get_height()/2.,
                f'{width:.4f}',
                ha='left', va='center', fontweight='bold', fontsize=11, 
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    plt.grid(axis='x', alpha=0.3, linestyle='--')
    plt.tight_layout()
    st.pyplot(fig3)
    

def render_model_testing(models, scaler, feature_names):
    """Render the model testing tab"""
    st.markdown(f'<h2 class="section-header">{SECTION_HEADERS["test_models"]}</h2>', 
                unsafe_allow_html=True)
    
    # Model selection
    col1, col2 = st.columns([2, 1])
    with col1:
        test_model_name = st.selectbox(
            "Select Model for Testing",
            list(models.keys()),
            key="test_model_select"
        )
    with col2:
        st.markdown(f"""
            <div style="background-color:#eff6ff; padding:1rem; border-radius:8px; margin-top:1.7rem;">
                <p style="margin:0; font-size:0.9rem; color:#1e40af;">
                    <strong>Selected:</strong> {test_model_name}
                </p>
            </div>
        """, unsafe_allow_html=True)
    
    selected_test_model = models[test_model_name]
    
    # File upload section
    st.markdown(f"### {SECTION_HEADERS['upload_data']}")
    st.markdown(f'<div class="info-box"><p style="margin:0;">{FILE_UPLOAD_INFO}</p></div>', 
               unsafe_allow_html=True)
    
    uploaded_file = st.file_uploader("Choose a CSV file", type=['csv'], key="test_file")
    
    if uploaded_file is not None:
        try:
            test_df = pd.read_csv(uploaded_file)
            
            st.markdown(f"""
                <div class="success-box">
                    <p style="margin:0;">
                        {UI_MESSAGES['file_upload_success']}<br>
                        Shape: <code>{test_df.shape[0]} rows √ó {test_df.shape[1]} columns</code>
                    </p>
                </div>
            """, unsafe_allow_html=True)
            
            with st.expander("üëÄ Preview Data (First 10 rows)"):
                st.dataframe(test_df.head(10), use_container_width=True)
            
            if st.button("üöÄ Run Prediction", key="predict_btn"):
                with st.spinner(UI_MESSAGES['processing']):
                    try:
                        X_test, y_true = preprocess_uploaded_data(test_df, scaler, feature_names)
                        predictions, probabilities = predict_with_model(selected_test_model, X_test)
                        
                        results_df = test_df.copy()
                        results_df['Prediction'] = [PREDICTION_LABELS[p] for p in predictions]
                        
                        if probabilities is not None:
                            results_df['Confidence_No'] = (probabilities[:, 0] * 100).round(2)
                            results_df['Confidence_Yes'] = (probabilities[:, 1] * 100).round(2)
                        
                        st.markdown(f'<h3 class="section-header">{SECTION_HEADERS["prediction_results"]}</h3>', 
                                   unsafe_allow_html=True)
                        
                        # Summary statistics
                        pred_counts = pd.Series(predictions).value_counts()
                        total = len(predictions)
                        
                        sum_col1, sum_col2, sum_col3 = st.columns(3)
                        with sum_col1:
                            st.metric("üìä Total Predictions", total)
                        with sum_col2:
                            st.metric("‚úÖ Predicted Yes", 
                                    f"{pred_counts.get(1, 0)} ({pred_counts.get(1, 0)/total*100:.1f}%)")
                        with sum_col3:
                            st.metric("‚ùå Predicted No", 
                                    f"{pred_counts.get(0, 0)} ({pred_counts.get(0, 0)/total*100:.1f}%)")
                        
                        st.dataframe(results_df.head(20), use_container_width=True)
                        
                        csv = results_df.to_csv(index=False)
                        st.download_button(
                            label="üì• Download Full Predictions as CSV",
                            data=csv,
                            file_name=f"{test_model_name.replace(' ', '_')}_predictions.csv",
                            mime="text/csv"
                        )
                        
                        if y_true is not None:
                            st.markdown(f'<h3 class="section-header">{SECTION_HEADERS["model_evaluation"]}</h3>', 
                                       unsafe_allow_html=True)
                            
                            metrics, cm, cr = calculate_metrics(y_true, predictions, probabilities)
                            display_metric_cards(metrics)
                            
                            eval_col1, eval_col2 = st.columns(2)
                            
                            with eval_col1:
                                st.markdown("#### üî¢ Confusion Matrix")
                                fig = plot_confusion_matrix(cm, f"{test_model_name}")
                                st.pyplot(fig)
                            
                            with eval_col2:
                                st.markdown("#### üìã Classification Report")
                                cr_df = pd.DataFrame(cr).transpose()
                                st.dataframe(cr_df.style.format("{:.4f}"), use_container_width=True)
                    
                    except Exception as e:
                        st.error(UI_MESSAGES['prediction_error'].format(str(e)))
                        st.info(UI_MESSAGES['ensure_features'])
        
        except Exception as e:
            st.error(UI_MESSAGES['file_load_error'].format(str(e)))
    
    else:
        req = TEST_DATA_REQUIREMENTS
        st.markdown(f"""
            <div class="info-box">
                <h4 style="margin:0 0 0.5rem 0;">{req['title']}</h4>
                <p style="margin:0;">{req['description']}</p>
                <code style="display:block; background:#f1f5f9; padding:0.5rem; border-radius:4px; margin-top:0.5rem;">
                """ + ", ".join(feature_names) + """
                </code>
                <p style="margin-top:0.5rem; font-size:0.9rem;">
                """ + req['optional_note'] + """
                </p>
            </div>
        """, unsafe_allow_html=True)

def main():
    # Header
    st.markdown(f'<h1 class="main-title">üéì {APP_CONFIG["title"]}</h1>', 
                unsafe_allow_html=True)
    
    # Load models
    try:
        model_data = load_models()
    except Exception as e:
        st.error(f"Error loading models: {str(e)}")
        import traceback
        with st.expander("View Full Error"):
            st.code(traceback.format_exc())
        model_data = None
    
    if model_data is None:
        st.error(UI_MESSAGES['models_not_found'])
        st.info(UI_MESSAGES['models_not_found_hint'])
        
        # Additional debug
        st.warning("‚ö†Ô∏è Debug: load_models() returned None even though all_models.pkl exists!")
        st.info("üí° This likely means your load_models() function in src/utils.py is looking for individual model files instead of all_models.pkl")
        return
    
    models = model_data['models']
    scaler = model_data['scaler']
    feature_names = model_data['feature_names']
    saved_metrics = model_data['metrics']
    saved_cm = model_data['confusion_matrices']
    
    # Top Navigation Tabs
    tab1, tab2 = st.tabs([TAB_NAMES['training'], TAB_NAMES['testing']])
    
    with tab1:
        render_training_results(models, scaler, feature_names, saved_metrics, saved_cm)
    
    with tab2:
        render_model_testing(models, scaler, feature_names)

if __name__ == "__main__":
    main()
